{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def compute_metrics(all_predictions,all_actuals, scaler, Normalization = False):    \n",
    "    mae = torch.mean(torch.abs(all_predictions - all_actuals)).item()\n",
    "    rmse = torch.sqrt(torch.mean((all_predictions - all_actuals) ** 2)).item()\n",
    "    print(f\"MAE in scaled space: {mae:.4f}\")\n",
    "    print(f\"RMSE in scaled space: {rmse:.4f}\")\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    all_predictions_np = all_predictions.detach().numpy()\n",
    "    all_actuals_np = all_actuals.detach().numpy()\n",
    "\n",
    "    # 'scaler' is already fitted on the training data\n",
    "    if Normalization:\n",
    "        all_predictions_original = scaler.inverse_transform(all_predictions_np)\n",
    "        all_actuals_original = scaler.inverse_transform(all_actuals_np)\n",
    "    else:\n",
    "        all_predictions_original = all_predictions_np\n",
    "        all_actuals_original = all_actuals_np\n",
    "    \n",
    "\n",
    "    # Compute metrics in the original scale\n",
    "    mae = np.mean(np.abs(all_predictions_original - all_actuals_original))\n",
    "    rmse = np.sqrt(np.mean((all_predictions_original - all_actuals_original) ** 2))\n",
    "\n",
    "    range_actuals = np.max(all_actuals_original) - np.min(all_actuals_original)\n",
    "    nmae = mae / range_actuals\n",
    "    nrmse = rmse / range_actuals\n",
    "\n",
    "    metrics = {'MAE':[mae], 'RMSE':[rmse], 'NMAE':[nmae], 'NRMSE':[nrmse]}\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sequentialdataset import SequentialDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def split_data(df, config_model):\n",
    "    ds, target = df.columns\n",
    "    test_size, input_seq_length, output_seq_length = config_model['test_size'], config_model['input_seq_length'], config_model['output_seq_length']\n",
    "    \n",
    "    train_data, test_data = train_test_split(df, test_size=test_size, shuffle=False)  # 15% as testing, (15%) as validation\n",
    "    \n",
    "    train_data_values = train_data[target].values.reshape(-1, 1)\n",
    "    test_data_values = test_data[target].values.reshape(-1, 1)\n",
    "\n",
    "    train_timestamps = train_data[ds].tolist()\n",
    "    test_timestamps = test_data[ds].tolist()\n",
    "\n",
    "    # Initialize and fit the scaler on training data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_data_scaled = scaler.fit_transform(train_data_values)\n",
    "\n",
    "    # Transform validation and test data using the same scaler\n",
    "    test_data_scaled = scaler.transform(test_data_values)\n",
    "\n",
    "    # Convert back to lists for dataset compatibility\n",
    "    train_data_scaled = train_data_scaled.flatten().tolist()\n",
    "    test_data_scaled = test_data_scaled.flatten().tolist()\n",
    "\n",
    "    train_val_dataset = SequentialDataset(train_data_scaled, train_timestamps, input_window=input_seq_length, output_window=output_seq_length)\n",
    "    test_dataset = SequentialDataset(test_data_scaled, test_timestamps, input_window=input_seq_length, output_window=output_seq_length)\n",
    "\n",
    "    train_size = int(df.shape[0] * (1-2*test_size))  # 70% for training and 15% for validation\n",
    "    train_dataset = torch.utils.data.Subset(train_val_dataset, range(0, train_size))\n",
    "    val_dataset = torch.utils.data.Subset(train_val_dataset, range(train_size, len(train_val_dataset)))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config_model['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config_model['batch_size'], shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config_model['batch_size'], shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, val_loader, test_dataset, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_helper import plot_sequence_predictions\n",
    "from datetime import date\n",
    "import os\n",
    "\n",
    "def save_seq_predictions(all_predictions, all_actuals, all_inputs, test_dataset, scaler, output_length, result_dir, target, normalization=True):\n",
    "    \n",
    "    today = date.today()\n",
    "    result_path=f'{result_dir}/images/TSTransformer/{today}/{target}'\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "    \n",
    "\n",
    "    result_data_path = f'{result_dir}/data/{today}_TSTransformer/{target}'\n",
    "    os.makedirs(result_data_path, exist_ok=True)\n",
    "\n",
    "    predictions_np = all_predictions.detach().cpu().numpy()\n",
    "    actuals_np = all_actuals.detach().cpu().numpy()\n",
    "    inputs_np = all_inputs.detach().cpu().numpy()\n",
    "    \n",
    "    if normalization:\n",
    "        predictions_np = scaler.inverse_transform(predictions_np)\n",
    "        actuals_np = scaler.inverse_transform(actuals_np)\n",
    "        inputs_np = scaler.inverse_transform(inputs_np)\n",
    "\n",
    "    for k in range (len(test_dataset)):\n",
    "        input_sequence=inputs_np[k]\n",
    "        actual_sequence = actuals_np[k]\n",
    "        predicted_sequence = predictions_np[k]\n",
    "\n",
    "        seq_x_timestamps, seq_y_timestamps = test_dataset.get_datetime_sequences(k)\n",
    "        seq_x_timestamps = pd.to_datetime(seq_x_timestamps)\n",
    "        seq_y_timestamps = pd.to_datetime(seq_y_timestamps)\n",
    "        timestamps = seq_x_timestamps.append(seq_y_timestamps).date\n",
    "\n",
    "        filename = f\"{result_data_path}/{k:03d}.csv\"\n",
    "        input_actual = np.append(input_sequence,actual_sequence)\n",
    "        input_predict = np.append(input_sequence,predicted_sequence)\n",
    "        results_df = pd.DataFrame({'timestamps':timestamps, 'actual':np.round(input_actual,3), 'predict': np.round(input_predict, 3)})\n",
    "        results_df.to_csv(filename, index = False)\n",
    "        if (k%14==0):\n",
    "            plot_sequence_predictions(input_sequence, actual_sequence, predicted_sequence, timestamps, target, result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.pvtw_tsf_transformer import PVTWTimeSeriesTransformer\n",
    "def window_analysis(df, config_model, feature, result_data_path):\n",
    "    all_metrics=[]\n",
    "\n",
    "    for window in range(7,42,7):\n",
    "        config_model['input_seq_length']  = 2*window\n",
    "        config_model['output_seq_length']  = window\n",
    "            \n",
    "        train_loader, test_loader, val_loader, test_dataset, scaler = split_data(df, config_model)\n",
    "        \n",
    "        model_path=f'./results/models/TSTransformer_{feature}_{window:03d}.pth'\n",
    "        pvtw_model = PVTWTimeSeriesTransformer(config_model)    \n",
    "        pvtw_model.fit(train_loader, val_loader)    \n",
    "        pvtw_model.save_model(model_path)\n",
    "        \n",
    "        all_predictions, all_actuals, all_inputs = pvtw_model.evaluate(test_loader) \n",
    "        metrics = compute_metrics(all_predictions,all_actuals, scaler, Normalization = True)    \n",
    "        all_metrics.append(metrics) \n",
    "    \n",
    "    metrics_df = pd.concat(all_metrics, ignore_index=True)\n",
    "    metrics_df.index = list(range(7,42,7))\n",
    "    metrics_df.round(6).to_csv(f'{result_data_path}/{feature}_TST_WindowAnalysis.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import date\n",
    "from scripts.pvtw_tsf_transformer import PVTWTimeSeriesTransformer\n",
    "\n",
    "data_path='./data/processed/' \n",
    "result_dir=f'./results/'\n",
    "today = date.today()\n",
    "result_data_path = f'{result_dir}/data/{today}_TSTransformer/'\n",
    "os.makedirs(result_data_path, exist_ok=True)\n",
    "\n",
    "gra = 'daily'\n",
    "features = ['fatalities', 'event']\n",
    "output_length= 28\n",
    "all_metrics=[]\n",
    "\n",
    "config_model = {'input_seq_length' :2*output_length, 'output_seq_length' : output_length, \n",
    "                'batch_size' :32,'epochs':50,\n",
    "                \"input_dim\": 1, \"output_dim\": 1,\n",
    "                \"model_dim\": 64,\"num_heads\": 4, \"ff_dim\": 128,\n",
    "                \"num_encoder_layers\": 3, \"num_decoder_layers\": 3,            \n",
    "                \"threshold\":  0, \"dropout\": 0.1, \"test_size\": 0.15\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "for feature in features:\n",
    "\n",
    "  filename=f\"{data_path}ts_pvtw_{gra}_{feature}.csv\"\n",
    "\n",
    "  df = pd.read_csv(filename) \n",
    "  ds, target = df.columns\n",
    "  df[ds] = pd.to_datetime(df[ds]) \n",
    "\n",
    "  threshold = df[target].quantile(0.75)  \n",
    "  config_model['threhold'] = threshold  \n",
    "    \n",
    "  train_loader, test_loader, val_loader, test_dataset, scaler = split_data(df, config_model)\n",
    "\n",
    "  model_path=f'./results/models/TSTransformer_{feature}.pth'\n",
    "  pvtw_model = PVTWTimeSeriesTransformer(config_model)    \n",
    "  pvtw_model.fit(train_loader, val_loader)    \n",
    "  pvtw_model.save_model(model_path)\n",
    "\n",
    "  all_predictions, all_actuals, all_inputs = pvtw_model.evaluate(test_loader) \n",
    "  metrics = compute_metrics(all_predictions,all_actuals, scaler, Normalization = True)    \n",
    "  all_metrics.append(metrics) \n",
    "\n",
    "  save_seq_predictions(all_predictions, all_actuals, all_inputs, test_dataset, scaler, output_length, result_dir, feature, normalization=True)\n",
    "  window_analysis(df, config_model, feature, result_data_path)\n",
    "\n",
    "metrics_df = pd.concat(all_metrics, ignore_index=True)\n",
    "metrics_df.index = [f\"{gra}--{feature}\" for feature in features]\n",
    "metrics_df.round(6).to_csv(f'{result_data_path}/metrics_TSTransformer.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
